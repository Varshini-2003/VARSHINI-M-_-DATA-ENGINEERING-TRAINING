{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d8JQTkCtUwfF"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CombiningData\").getOrCreate()\n",
        "\n",
        "# Employee Data\n",
        "employee_data = [\n",
        "    (\"Ananya\", \"HR\", 50000),\n",
        "    (\"Rahul\", \"Engineering\", 70000),\n",
        "    (\"Priya\", \"Engineering\", 65000),\n",
        "    (\"Zoya\", \"Marketing\", 48000),\n",
        "    (\"Karan\", \"HR\", 52000),\n",
        "    (\"Naveen\", \"Engineering\", 72000),\n",
        "    (\"Fatima\", \"Marketing\", 46000)\n",
        "]\n",
        "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
        "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
        "\n",
        "# Performance Data\n",
        "performance_data = [\n",
        "    (\"Ananya\", 4.5),\n",
        "    (\"Rahul\", 4.8),\n",
        "    (\"Priya\", 4.1),\n",
        "    (\"Zoya\", 3.9),\n",
        "    (\"Karan\", 4.2),\n",
        "    (\"Naveen\", 4.9),\n",
        "    (\"Fatima\", 3.7)\n",
        "]\n",
        "columns_perf = [\"Name\", \"Rating\"]\n",
        "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
        "\n",
        "# Project Data\n",
        "project_data = [\n",
        "    (\"Ananya\", \"HR Portal\", 120),\n",
        "    (\"Rahul\", \"Data Platform\", 200),\n",
        "    (\"Priya\", \"Data Platform\", 180),\n",
        "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
        "    (\"Karan\", \"HR Portal\", 130),\n",
        "    (\"Naveen\", \"ML Pipeline\", 220),\n",
        "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
        "]\n",
        "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
        "df_proj = spark.createDataFrame(project_data, columns_proj)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Joins and Advanced Aggregations"
      ],
      "metadata": {
        "id": "7n-dzUY6VQSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Join employee_data, performance_data, and project_data\n",
        "df_joined = df_emp.join(df_perf, \"Name\").join(df_proj, \"Name\")\n",
        "df_joined.show()\n",
        "\n",
        "# 2. Compute total hours worked per department\n",
        "df_joined.groupBy(\"Department\").agg(sum(\"HoursWorked\").alias(\"TotalHoursWorked\")).show()\n",
        "\n",
        "# 3. Compute average rating per project\n",
        "df_joined.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AvgRating\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmxZCCNWVI8h",
        "outputId": "ddf13f84-9e64-49ec-8350-3169e55ff09b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+------+----------------+-----------+\n",
            "|  Name| Department|Salary|Rating|         Project|HoursWorked|\n",
            "+------+-----------+------+------+----------------+-----------+\n",
            "|Ananya|         HR| 50000|   4.5|       HR Portal|        120|\n",
            "| Priya|Engineering| 65000|   4.1|   Data Platform|        180|\n",
            "| Rahul|Engineering| 70000|   4.8|   Data Platform|        200|\n",
            "|Naveen|Engineering| 72000|   4.9|     ML Pipeline|        220|\n",
            "|Fatima|  Marketing| 46000|   3.7|Campaign Tracker|         90|\n",
            "|  Zoya|  Marketing| 48000|   3.9|Campaign Tracker|        100|\n",
            "| Karan|         HR| 52000|   4.2|       HR Portal|        130|\n",
            "+------+-----------+------+------+----------------+-----------+\n",
            "\n",
            "+-----------+----------------+\n",
            "| Department|TotalHoursWorked|\n",
            "+-----------+----------------+\n",
            "|Engineering|             600|\n",
            "|         HR|             250|\n",
            "|  Marketing|             190|\n",
            "+-----------+----------------+\n",
            "\n",
            "+----------------+-----------------+\n",
            "|         Project|        AvgRating|\n",
            "+----------------+-----------------+\n",
            "|   Data Platform|4.449999999999999|\n",
            "|       HR Portal|             4.35|\n",
            "|     ML Pipeline|              4.9|\n",
            "|Campaign Tracker|              3.8|\n",
            "+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Handling Missing Data (introduce some manually)"
      ],
      "metadata": {
        "id": "gMFSIGDlVZdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "\n",
        "# Define schema for performance_data\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Rating\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# 4. Add a row with None rating\n",
        "new_row = spark.createDataFrame([(\"John\", None)], schema=schema)\n",
        "\n",
        "# Union with existing df_perf\n",
        "df_perf_updated = df_perf.union(new_row)\n",
        "df_perf_updated.show()\n",
        "\n",
        "\n",
        "# 5. Filter rows with null values\n",
        "df_perf_updated.filter(col(\"Rating\").isNull()).show()\n",
        "\n",
        "# 6. Replace null ratings with the department average\n",
        "df_perf_with_dept = df_perf_updated.join(df_emp, \"Name\", \"left\")\n",
        "\n",
        "avg_dept_rating = df_perf_with_dept.groupBy(\"Department\") \\\n",
        "    .agg(avg(\"Rating\").alias(\"AvgDeptRating\"))\n",
        "\n",
        "df_filled = df_perf_with_dept.join(avg_dept_rating, \"Department\", \"left\") \\\n",
        "    .withColumn(\"FinalRating\", when(col(\"Rating\").isNull(), col(\"AvgDeptRating\")).otherwise(col(\"Rating\"))) \\\n",
        "    .select(\"Name\", \"Department\", \"FinalRating\")\n",
        "\n",
        "df_filled.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRL4iBM8VdYa",
        "outputId": "23b49e82-ef92-40be-c339-4ba79347bcf9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|  Name|Rating|\n",
            "+------+------+\n",
            "|Ananya|   4.5|\n",
            "| Rahul|   4.8|\n",
            "| Priya|   4.1|\n",
            "|  Zoya|   3.9|\n",
            "| Karan|   4.2|\n",
            "|Naveen|   4.9|\n",
            "|Fatima|   3.7|\n",
            "|  John|  NULL|\n",
            "+------+------+\n",
            "\n",
            "+----+------+\n",
            "|Name|Rating|\n",
            "+----+------+\n",
            "|John|  NULL|\n",
            "+----+------+\n",
            "\n",
            "+------+-----------+-----------+\n",
            "|  Name| Department|FinalRating|\n",
            "+------+-----------+-----------+\n",
            "|Ananya|         HR|        4.5|\n",
            "| Priya|Engineering|        4.1|\n",
            "| Rahul|Engineering|        4.8|\n",
            "|Naveen|Engineering|        4.9|\n",
            "|Fatima|  Marketing|        3.7|\n",
            "|  Zoya|  Marketing|        3.9|\n",
            "| Karan|         HR|        4.2|\n",
            "|  John|       NULL|       NULL|\n",
            "+------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Built-In Functions and UDF"
      ],
      "metadata": {
        "id": "HnKLx7GZWAQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, when, col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# 7. Create a column PerformanceCategory\n",
        "df_cat = df_joined.withColumn(\"PerformanceCategory\",\n",
        "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
        "    .when((col(\"Rating\") >= 4.0) & (col(\"Rating\") < 4.7), \"Good\")\n",
        "    .otherwise(\"Average\"))\n",
        "df_cat.select(\"Name\", \"Rating\", \"PerformanceCategory\").show()\n",
        "\n",
        "# 8. Create a UDF to assign bonus: If project hours > 200 → 10,000 else → 5,000\n",
        "def assign_bonus(hours):\n",
        "    return 10000 if hours > 200 else 5000\n",
        "\n",
        "bonus_udf = udf(assign_bonus, IntegerType())\n",
        "\n",
        "df_bonus = df_joined.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
        "df_bonus.select(\"Name\", \"HoursWorked\", \"Bonus\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjM_iIpaWIF7",
        "outputId": "d1f2dd0e-93dd-4e91-f3fb-d816845ad6ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------------------+\n",
            "|  Name|Rating|PerformanceCategory|\n",
            "+------+------+-------------------+\n",
            "|Ananya|   4.5|               Good|\n",
            "| Priya|   4.1|               Good|\n",
            "| Rahul|   4.8|          Excellent|\n",
            "|Naveen|   4.9|          Excellent|\n",
            "|Fatima|   3.7|            Average|\n",
            "|  Zoya|   3.9|            Average|\n",
            "| Karan|   4.2|               Good|\n",
            "+------+------+-------------------+\n",
            "\n",
            "+------+-----------+-----+\n",
            "|  Name|HoursWorked|Bonus|\n",
            "+------+-----------+-----+\n",
            "|Ananya|        120| 5000|\n",
            "| Priya|        180| 5000|\n",
            "| Rahul|        200| 5000|\n",
            "|Naveen|        220|10000|\n",
            "|Fatima|         90| 5000|\n",
            "|  Zoya|        100| 5000|\n",
            "| Karan|        130| 5000|\n",
            "+------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date and Time Functions"
      ],
      "metadata": {
        "id": "qFResnvTWbqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Add a column JoinDate with 2021-06-01 and calculate MonthsWorked\n",
        "df_with_date = df_joined.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
        "df_with_date = df_with_date.withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
        "df_with_date.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()\n",
        "\n",
        "# 10. Calculate how many employees joined before 2022\n",
        "count_before_2022 = df_with_date.filter(col(\"JoinDate\") < \"2022-01-01\").select(\"Name\").distinct().count()\n",
        "print(f\"Employees who joined before 2022: {count_before_2022}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMKngv3qWh75",
        "outputId": "626c7283-0b7f-4761-a1cd-f0a0f532f4d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+------------+\n",
            "|  Name|  JoinDate|MonthsWorked|\n",
            "+------+----------+------------+\n",
            "|Ananya|2021-06-01|          48|\n",
            "| Priya|2021-06-01|          48|\n",
            "| Rahul|2021-06-01|          48|\n",
            "|Naveen|2021-06-01|          48|\n",
            "|Fatima|2021-06-01|          48|\n",
            "|  Zoya|2021-06-01|          48|\n",
            "| Karan|2021-06-01|          48|\n",
            "+------+----------+------------+\n",
            "\n",
            "Employees who joined before 2022: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unions\n"
      ],
      "metadata": {
        "id": "pjdBVNnCWnWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Create another small team DataFrame and union it with employee_data\n",
        "extra_employees = [\n",
        "    (\"Meena\", \"HR\", 48000),\n",
        "    (\"Raj\", \"Marketing\", 51000)\n",
        "]\n",
        "df_extra = spark.createDataFrame(extra_employees, columns_emp)\n",
        "df_union = df_emp.union(df_extra)\n",
        "df_union.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPkHhwLoWrlF",
        "outputId": "ea330e26-f18b-4431-d800-4ccd09301085"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|Ananya|         HR| 50000|\n",
            "| Rahul|Engineering| 70000|\n",
            "| Priya|Engineering| 65000|\n",
            "|  Zoya|  Marketing| 48000|\n",
            "| Karan|         HR| 52000|\n",
            "|Naveen|Engineering| 72000|\n",
            "|Fatima|  Marketing| 46000|\n",
            "| Meena|         HR| 48000|\n",
            "|   Raj|  Marketing| 51000|\n",
            "+------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Saving Results"
      ],
      "metadata": {
        "id": "OC4oOcUWWyIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based on Department\n",
        "final_df = df_joined.select(\"Name\", \"Department\", \"Salary\", \"Rating\", \"Project\", \"HoursWorked\")\n",
        "final_df.write.partitionBy(\"Department\").parquet(\"/path/to/output/final_data.parquet\", mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "_RDciwpGWvwk"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}